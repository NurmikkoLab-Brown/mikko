{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mikko Neural Processing Toolkit Mikko (\"meeko\") is a neural processing toolkit designed for large-scale computational neuroscience experiments. This software is developed and maintained by the Nurmikko Lab at Brown University. Mikko is currently under development. Please check back regularly for new modules and updated documentation/examples. Table of Contents Installation Getting Started Documentation Citing mikko 1. Installation Mikko modules can be run as stand-alone containers or with Dockex. To run modules stand-alone, install Docker-CE . For GPU support, you must also install the appropriate NVIDIA drivers and install nvidia-docker . To run modules with Dockex, install Dockex . For GPU support, you must set the enable_gpus flag to true in the dockex configuration. Each module runs as a self-contained container. No additional installation is necessary. 2. Getting Started Stand-alone When running modules stand-alone, you must handle the following manually: build the module image define a configuration JSON file that defines parameters, input pathnames, and output pathnames create a data directory for passing input files and storing output files run the module as a docker container with a volume linking to the data directory To build stand-alone images, point to the module's Dockerfile. For example, cd /path/to/mikko docker build --tag lstm_rnn_image -f modules/decoders/keras/lstm_rnn/Dockerfile . The module can then be run by first defining a run configuration JSON file. See test.json files in each module directory for examples. When running stand-alone, you must also create a directory for storing the input and output data (e.g. /tmp/data ). Then run the module. The JSON filename should be passed as an argument to the module. For example, docker run -it --rm -v /tmp/data:/tmp/data lstm_rnn_image test.json Note that the JSON file must be accessible from within the docker container. This can be done by building the file into the image or attaching a volume containing the file to the container. Dockex To run modules with Dockex, create an experiment file (see experiments/basic_auditory_experiment.py ), and copy your input data to the tmp_dockex_path (e.g. /tmp/dockex/data ). Launch the experiment through the LAUNCH screen of the Dockex GUI. For example, Project Path: /path/to/mikko Experiment Path: experiments/basic_auditory_experiment.py Set the number of desired CPU/GPU credits using the MACHINES screen. The PROGRESS screen will indicate when the experiment is complete. Results will be saved to the tmp_dockex_path. 3. Documentation Documentation can be found here . 4. Citing mikko If you make use of this software for your work, please consider referencing the following: mikko @software{mikko, author = {ChrisHeelan}, title = {NurmikkoLab-Brown/mikko: Initial Release}, month = nov, year = 2019, publisher = {Zenodo}, version = {v0.1.1}, doi = {10.5281/zenodo.3525273}, url = {https://doi.org/10.5281/zenodo.3525273} }","title":"Home"},{"location":"#mikko-neural-processing-toolkit","text":"Mikko (\"meeko\") is a neural processing toolkit designed for large-scale computational neuroscience experiments. This software is developed and maintained by the Nurmikko Lab at Brown University. Mikko is currently under development. Please check back regularly for new modules and updated documentation/examples.","title":"mikko Neural Processing Toolkit"},{"location":"#table-of-contents","text":"Installation Getting Started Documentation Citing mikko","title":"Table of Contents"},{"location":"#1-installation","text":"Mikko modules can be run as stand-alone containers or with Dockex. To run modules stand-alone, install Docker-CE . For GPU support, you must also install the appropriate NVIDIA drivers and install nvidia-docker . To run modules with Dockex, install Dockex . For GPU support, you must set the enable_gpus flag to true in the dockex configuration. Each module runs as a self-contained container. No additional installation is necessary.","title":"1. Installation"},{"location":"#2-getting-started","text":"","title":"2. Getting Started"},{"location":"#stand-alone","text":"When running modules stand-alone, you must handle the following manually: build the module image define a configuration JSON file that defines parameters, input pathnames, and output pathnames create a data directory for passing input files and storing output files run the module as a docker container with a volume linking to the data directory To build stand-alone images, point to the module's Dockerfile. For example, cd /path/to/mikko docker build --tag lstm_rnn_image -f modules/decoders/keras/lstm_rnn/Dockerfile . The module can then be run by first defining a run configuration JSON file. See test.json files in each module directory for examples. When running stand-alone, you must also create a directory for storing the input and output data (e.g. /tmp/data ). Then run the module. The JSON filename should be passed as an argument to the module. For example, docker run -it --rm -v /tmp/data:/tmp/data lstm_rnn_image test.json Note that the JSON file must be accessible from within the docker container. This can be done by building the file into the image or attaching a volume containing the file to the container.","title":"Stand-alone"},{"location":"#dockex","text":"To run modules with Dockex, create an experiment file (see experiments/basic_auditory_experiment.py ), and copy your input data to the tmp_dockex_path (e.g. /tmp/dockex/data ). Launch the experiment through the LAUNCH screen of the Dockex GUI. For example, Project Path: /path/to/mikko Experiment Path: experiments/basic_auditory_experiment.py Set the number of desired CPU/GPU credits using the MACHINES screen. The PROGRESS screen will indicate when the experiment is complete. Results will be saved to the tmp_dockex_path.","title":"Dockex"},{"location":"#3-documentation","text":"Documentation can be found here .","title":"3. Documentation"},{"location":"#4-citing-mikko","text":"If you make use of this software for your work, please consider referencing the following:","title":"4. Citing mikko"},{"location":"#mikko","text":"@software{mikko, author = {ChrisHeelan}, title = {NurmikkoLab-Brown/mikko: Initial Release}, month = nov, year = 2019, publisher = {Zenodo}, version = {v0.1.1}, doi = {10.5281/zenodo.3525273}, url = {https://doi.org/10.5281/zenodo.3525273} }","title":"mikko"},{"location":"analysis_regression_metrics/","text":"analysis/regression_metrics This module calculates the mean correlation between target values and predicted values for training, validation, and test sets. Correlations are averaged across columns (i.e., axis=1 ) using the Fisher z-transformation to impose additivity and to approximate a normal sampling distribution. For an example JSON configuration file, please see test.json. Parameters None Input Pathnames targets_train_npy (numpy file): Required. 2D array of training set target values. predict_train_npy (numpy file): Required. 2D array of training set predicted values. targets_valid_npy (numpy file): Required. 2D array of validation set target values. predict_valid_npy (numpy file): Required. 2D array of validation set predicted values. targets_test_npy (numpy file): Required. 2D array of testing set target values. predict_test_npy (numpy file): Required. 2D array of testing set predicted values. Output Pathnames csv (CSV file): Contains the mean correlation for each dataset as well as lists of the correlations for each target.","title":"regression_metrics"},{"location":"analysis_regression_metrics/#analysisregression_metrics","text":"This module calculates the mean correlation between target values and predicted values for training, validation, and test sets. Correlations are averaged across columns (i.e., axis=1 ) using the Fisher z-transformation to impose additivity and to approximate a normal sampling distribution. For an example JSON configuration file, please see test.json. Parameters None Input Pathnames targets_train_npy (numpy file): Required. 2D array of training set target values. predict_train_npy (numpy file): Required. 2D array of training set predicted values. targets_valid_npy (numpy file): Required. 2D array of validation set target values. predict_valid_npy (numpy file): Required. 2D array of validation set predicted values. targets_test_npy (numpy file): Required. 2D array of testing set target values. predict_test_npy (numpy file): Required. 2D array of testing set predicted values. Output Pathnames csv (CSV file): Contains the mean correlation for each dataset as well as lists of the correlations for each target.","title":"analysis/regression_metrics"},{"location":"decoders_kalman_filter/","text":"decoders/kalman_filter Fit a Kalman filter decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . C (float): Required. Scaling value for the noise matrix. save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"kalman_filter"},{"location":"decoders_kalman_filter/#decoderskalman_filter","text":"Fit a Kalman filter decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . C (float): Required. Scaling value for the noise matrix. save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"decoders/kalman_filter"},{"location":"decoders_keras_dense_nn/","text":"decoders/keras/dense_nn Fit a single-layer (i.e., one hidden layer) dense neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of hidden layer units. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"dense_nn"},{"location":"decoders_keras_dense_nn/#decoderskerasdense_nn","text":"Fit a single-layer (i.e., one hidden layer) dense neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of hidden layer units. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"decoders/keras/dense_nn"},{"location":"decoders_keras_gru_rnn/","text":"decoders/keras/gru_rnn Fit a single-layer GRU recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of GRU cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"gru_rnn"},{"location":"decoders_keras_gru_rnn/#decoderskerasgru_rnn","text":"Fit a single-layer GRU recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of GRU cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"decoders/keras/gru_rnn"},{"location":"decoders_keras_lstm_rnn/","text":"decoders/keras/lstm_rnn Fit a single-layer LSTM recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of LSTM cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"lstm_rnn"},{"location":"decoders_keras_lstm_rnn/#decoderskeraslstm_rnn","text":"Fit a single-layer LSTM recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of LSTM cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"decoders/keras/lstm_rnn"},{"location":"decoders_keras_simple_rnn/","text":"decoders/keras/simple_rnn Fit a single-layer simple recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of recurrent cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"simple_rnn"},{"location":"decoders_keras_simple_rnn/#decoderskerassimple_rnn","text":"Fit a single-layer simple recurrent neural network decoder for regression and make predictions. Implemented using Keras with a TensorFlow backend. Training utilizes a validation set to perform early stopping based on validation loss. This module is built using a GPU-enabled docker image. For more information, see here . For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . units (int): Required. Number of recurrent cells. batch_size (int): Training batch size. Default = None . Note that Keras will default to 32 . epochs (int): Maximum number of training epochs. Default = 2048 . save_model (bool): Set to true to save the fitted model as a keras HDF5 file . Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_keras ( keras HDF5 file ): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_keras ( keras HDF5 file ): Saved trained model.","title":"decoders/keras/simple_rnn"},{"location":"decoders_wiener_cascade/","text":"decoders/wiener_cascade Fit a Wiener cascade decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . degree (int): Required. Polynomial degree. save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"wiener_cascade"},{"location":"decoders_wiener_cascade/#decoderswiener_cascade","text":"Fit a Wiener cascade decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . degree (int): Required. Polynomial degree. save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"decoders/wiener_cascade"},{"location":"decoders_wiener_filter/","text":"decoders/wiener_filter Fit a Wiener filter decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"wiener_filter"},{"location":"decoders_wiener_filter/#decoderswiener_filter","text":"Fit a Wiener filter decoder for regression and make predictions. This module is based on code from the Kording Lab Neural_Decoding repo. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Method to call of the model. Must be fit , fit_predict , or predict . save_model (bool): Set to true to save the fitted model as a joblib file. Only utilized if model is not loaded from file (i.e., method is not predict ). Default = false . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . Input Pathnames X_train_npy (numpy file): Required. 2D array of training set features. y_train_npy (numpy file): Required. 2D array of training set targets. X_valid_npy (numpy file): Required. 2D array of validation set features. y_valid_npy (numpy file): Required. 2D array of validation set targets. X_test_npy (numpy file): Required. 2D array of testing set features. y_test_npy (numpy file): Required. 2D array of testing set targets. model_joblib (joblib file): Pretrained model to load. Only utilized if method is predict . Output Pathnames predict_train_npy (numpy file): 2D array of training set predictions. predict_valid_npy (numpy file): 2D array of validation set predictions. predict_test_npy (numpy file): 2D array of testing set predictions. model_joblib (joblib file): Saved trained model.","title":"decoders/wiener_filter"},{"location":"preprocessing_create_window_features/","text":"preprocessing/create_window_features Converts sequential 2D features of size samples x num_features into windows of features of size samples x window_size x num_features . This 3D form is useful when utilizing recurrent neural networks. Additionally, standard \"flat\" arrays are generated that concatenate all windows across features for a given sample to create an array of size samples x (window_size * num_features) . Windows are centered on the current sample and may stretch forwards and/or backwards in time. Samples that cannot form a complete window (beginning or end of the dataset) are dropped automatically. This module requires the corresponding targets dataset as well to ensure samples stay index-matched across features and targets. For an example JSON configuration file, please see test.json. Parameters samples_before (int): Required. Include this many samples before the current sample in the window. samples_after (int): Required. Include this many samples after the current sample in the window. use_current_sample (bool): Required. Whether to include the current sample in the window. Input Pathnames features_npy (numpy file): Required. 2D features array. targets_npy (numpy file): Required. Targets array. Note that the features and targets arrays should be index-matched by row (i.e., axis=0 ). Output Pathnames window_features_npy (numpy file): 3D features array of size samples x window_size x num_features . window_targets_npy (numpy file): Targets array corresponding to window_features_array. flat_window_features_npy (numpy file): 2D features array of size samples x (window_size * num_features) . flat_window_targets_npy (numpy file): Targets array corresponding to flat_window_features_npy.","title":"create_window_features"},{"location":"preprocessing_create_window_features/#preprocessingcreate_window_features","text":"Converts sequential 2D features of size samples x num_features into windows of features of size samples x window_size x num_features . This 3D form is useful when utilizing recurrent neural networks. Additionally, standard \"flat\" arrays are generated that concatenate all windows across features for a given sample to create an array of size samples x (window_size * num_features) . Windows are centered on the current sample and may stretch forwards and/or backwards in time. Samples that cannot form a complete window (beginning or end of the dataset) are dropped automatically. This module requires the corresponding targets dataset as well to ensure samples stay index-matched across features and targets. For an example JSON configuration file, please see test.json. Parameters samples_before (int): Required. Include this many samples before the current sample in the window. samples_after (int): Required. Include this many samples after the current sample in the window. use_current_sample (bool): Required. Whether to include the current sample in the window. Input Pathnames features_npy (numpy file): Required. 2D features array. targets_npy (numpy file): Required. Targets array. Note that the features and targets arrays should be index-matched by row (i.e., axis=0 ). Output Pathnames window_features_npy (numpy file): 3D features array of size samples x window_size x num_features . window_targets_npy (numpy file): Targets array corresponding to window_features_array. flat_window_features_npy (numpy file): 2D features array of size samples x (window_size * num_features) . flat_window_targets_npy (numpy file): Targets array corresponding to flat_window_features_npy.","title":"preprocessing/create_window_features"},{"location":"preprocessing_sklearn_preprocess/","text":"preprocessing/sklearn_preprocess This module exposes the fit , fit_transform , and transform methods for any transformer in the the scikit-learn preprocessing library. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Must be fit , fit_transform , transform , or inverse_transform . class (str): Required if method is fit or fit_transform . The class of the scikit preprocessing transformer. Must be located in sklearn.preprocessing . kwargs (dictionary): The kwargs passed to the transformer. train_decimal (float or null): If not null, the decimal fraction of input data to use to build the transformer. Data is taken from the beginning of the array. save_model (bool): If true , the fitted transformer will be stored to disk. random_seed (int): Random seed for numpy and random. Input Pathnames X_npy (numpy file): Required. The input array passed to the method . model_joblib (joblib file): A previously saved transformer for use with the transform and inverse_transform methods. Output Pathnames output_npy (numpy file): The array returned by the method if one is returned. model_joblib (joblib file): If save_model is true , the saved fitted transformer.","title":"sklearn_preprocess"},{"location":"preprocessing_sklearn_preprocess/#preprocessingsklearn_preprocess","text":"This module exposes the fit , fit_transform , and transform methods for any transformer in the the scikit-learn preprocessing library. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Must be fit , fit_transform , transform , or inverse_transform . class (str): Required if method is fit or fit_transform . The class of the scikit preprocessing transformer. Must be located in sklearn.preprocessing . kwargs (dictionary): The kwargs passed to the transformer. train_decimal (float or null): If not null, the decimal fraction of input data to use to build the transformer. Data is taken from the beginning of the array. save_model (bool): If true , the fitted transformer will be stored to disk. random_seed (int): Random seed for numpy and random. Input Pathnames X_npy (numpy file): Required. The input array passed to the method . model_joblib (joblib file): A previously saved transformer for use with the transform and inverse_transform methods. Output Pathnames output_npy (numpy file): The array returned by the method if one is returned. model_joblib (joblib file): If save_model is true , the saved fitted transformer.","title":"preprocessing/sklearn_preprocess"},{"location":"preprocessing_train_valid_test_split/","text":"preprocessing/train_valid_test_split This module splits index-matched target and feature arrays into training, validation, and test sets. The data is split sequentially along the rows (i.e., axis=0 ). For an example JSON configuration file, please see test.json. Parameters train_decimal (float): Required. Training data decimal fraction. valid_decimal (float): Required. Validation data decimal fraction. test_decimal (float): Required. Testing data decimal fraction. Note that train_decimal + valid_decimal + test_decimal <= 1.0 Input Pathnames features_npy (numpy file): Required. Features array. targets_npy (numpy file): Required. Targets array. Note that the features and targets arrays should be index-matched by row (i.e., axis=0 ). Output Pathnames features_train_npy (numpy file): Training set features. targets_train_npy (numpy file): Training set targets. features_valid_npy (numpy file): Validation set features. targets_valid_npy (numpy file): Validation set targets. features_test_npy (numpy file): Testing set features. targets_test_npy (numpy file): Testing set targets.","title":"train_valid_test_split"},{"location":"preprocessing_train_valid_test_split/#preprocessingtrain_valid_test_split","text":"This module splits index-matched target and feature arrays into training, validation, and test sets. The data is split sequentially along the rows (i.e., axis=0 ). For an example JSON configuration file, please see test.json. Parameters train_decimal (float): Required. Training data decimal fraction. valid_decimal (float): Required. Validation data decimal fraction. test_decimal (float): Required. Testing data decimal fraction. Note that train_decimal + valid_decimal + test_decimal <= 1.0 Input Pathnames features_npy (numpy file): Required. Features array. targets_npy (numpy file): Required. Targets array. Note that the features and targets arrays should be index-matched by row (i.e., axis=0 ). Output Pathnames features_train_npy (numpy file): Training set features. targets_train_npy (numpy file): Training set targets. features_valid_npy (numpy file): Validation set features. targets_valid_npy (numpy file): Validation set targets. features_test_npy (numpy file): Testing set features. targets_test_npy (numpy file): Testing set targets.","title":"preprocessing/train_valid_test_split"},{"location":"tasks_auditory_librosa_mel_transformer/","text":"tasks/auditory/librosa_mel_transformer Transforms audio data into its mel-spectrogram representation or transforms a mel-spectrogram into audio. The Griffin-Lim algorithm is used for reconstructing phase information when transforming a mel-spectrogram to audio. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Must be mel_transform or inverse_mel_transform . sampling_rate (float): Default = 30000.0 . ref_level_db (int): Audio reference level in dB. Default = 20 . power (float): Value applied to spectrogram before Griffin-Lim. Default = 1.5 . griffin_lim_iters (int): Number of iterations for Griffin-Lim. Default = 60 . fft_size (int): Length of fft. Default = 2048 . num_mels (int): Number of mel-bands. Default = 80 . preemphasis (float): Preemphasis applied before mel-conversion and after Griffin-Lim. Default = 0.97 . max_abs_value (int): Max absolute value for normalization. Default = 4 . min_level_db (int): Min level in dB for normalization. Default = -100 . frame_shift_ms (float): Shift size in milliseconds. Default = 20.0 . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . save_transformer (bool): If true , the fitted transformer will be stored to disk. Default = false . Input Pathnames input (numpy or HDF5 file): Required. Either audio data or mel-spectrogram array. HDF5 file must have data stored at key data . transformer_joblib (joblib file): A previously saved transformer. Output Pathnames mel_h5 (HDF5 file): Generated mel-spectrogram HDF5 stored at key data . Only generated if method is mel_transform and input was an HDF5 file. mel_npy (numpy file): Generated mel-spectrogram array. Only generated if method is mel_transform and input was a numpy array. audio_wav (WAV file): Generated audio file. Only generated if method is inverse_mel_transform . transformer_joblib (joblib file): If save_transformer is true , the saved transformer.","title":"librosa_mel_transformer"},{"location":"tasks_auditory_librosa_mel_transformer/#tasksauditorylibrosa_mel_transformer","text":"Transforms audio data into its mel-spectrogram representation or transforms a mel-spectrogram into audio. The Griffin-Lim algorithm is used for reconstructing phase information when transforming a mel-spectrogram to audio. For an example JSON configuration file, please see test.json. Parameters method (str): Required. Must be mel_transform or inverse_mel_transform . sampling_rate (float): Default = 30000.0 . ref_level_db (int): Audio reference level in dB. Default = 20 . power (float): Value applied to spectrogram before Griffin-Lim. Default = 1.5 . griffin_lim_iters (int): Number of iterations for Griffin-Lim. Default = 60 . fft_size (int): Length of fft. Default = 2048 . num_mels (int): Number of mel-bands. Default = 80 . preemphasis (float): Preemphasis applied before mel-conversion and after Griffin-Lim. Default = 0.97 . max_abs_value (int): Max absolute value for normalization. Default = 4 . min_level_db (int): Min level in dB for normalization. Default = -100 . frame_shift_ms (float): Shift size in milliseconds. Default = 20.0 . random_seed (int): Random seed for numpy and random libraries. Default = 1337 . save_transformer (bool): If true , the fitted transformer will be stored to disk. Default = false . Input Pathnames input (numpy or HDF5 file): Required. Either audio data or mel-spectrogram array. HDF5 file must have data stored at key data . transformer_joblib (joblib file): A previously saved transformer. Output Pathnames mel_h5 (HDF5 file): Generated mel-spectrogram HDF5 stored at key data . Only generated if method is mel_transform and input was an HDF5 file. mel_npy (numpy file): Generated mel-spectrogram array. Only generated if method is mel_transform and input was a numpy array. audio_wav (WAV file): Generated audio file. Only generated if method is inverse_mel_transform . transformer_joblib (joblib file): If save_transformer is true , the saved transformer.","title":"tasks/auditory/librosa_mel_transformer"}]}